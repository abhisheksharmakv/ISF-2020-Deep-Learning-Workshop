{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Forecasting using GluonTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is GluonTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GluonTS is an open source Python library for time series modeling, with a focus on deep learning architectures for forecasting.\n",
    "It provides several tools to support development and experimentation with new models, as well as pre-implemented models from the literature.\n",
    "We originally developed GluonTS on top of MXNet's Gluon API (hence the name), but we're making all parts of the library compatible with PyTorch models too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target users:\n",
    "* **Researchers** who want to try out novel architectures for forecasting and compare them to the state of the art\n",
    "* **Data scientist/solution architects** who want to experiment with several solution to solve business problems\n",
    "* **Machine learning engineer** who need to integrate forecasting models in production pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you're going to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Handle datasets in GluonTS (transform them and iterate them)\n",
    "* Forecast and do backtest evaluations given a model (\"predictors\")\n",
    "* Implement a neural-network-based architecture using PyTorch, and train it over a dataset\n",
    "* Use pre-implemented architectures (\"estimators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Dict\n",
    "from itertools import islice\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.repository.datasets import get_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we're going to use the hourly dataset from the M4 competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(\"m4_hourly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(next(iter(dataset.train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_series(entry: Dict) -> pd.Series:\n",
    "    index = pd.date_range(start=entry[\"start\"], periods=len(entry[\"target\"]), freq=entry[\"start\"].freq)\n",
    "    return pd.Series(entry[\"target\"], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for idx, entry in enumerate(islice(dataset.train, 9)):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "    to_series(entry).plot()\n",
    "    plt.grid()\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.seasonal_naive import SeasonalNaivePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_naive = SeasonalNaivePredictor(\n",
    "    freq=dataset.metadata.freq,\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    season_length=7 * 24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = list(predictor_naive.predict(dataset.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for idx, (entry, f) in islice(enumerate(zip(dataset.train, forecasts)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "    to_series(entry)[-3*24:].plot()\n",
    "    f.plot(color=\"r\")\n",
    "    plt.grid()\n",
    "    plt.title(f\"item_id: {f.item_id}\")\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.evaluation.backtest import backtest_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_metrics_naive, entrywise_metrics_naive = backtest_metrics(dataset.test, predictor_naive, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(\n",
    "    [aggregate_metrics_naive],\n",
    "    index=[\"Naive seasonal\"]\n",
    ").transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrywise_metrics_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing a PyTorch model: a probabilistic feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pretty simple model, based on a feed-forward network whose output layer produces the parameters of a Student's t-distribution at each time step in the prediction range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/feedforward.png\" style=\"width: 300px; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define two networks based on this idea:\n",
    "* The `TrainingFeedForwardNetwork` computes the loss associated with given observations, i.e. the negative log-likelihood of the observations according to the output distribution; this will be used during training.\n",
    "* The `SamplingFeedForwardNetwork` will be used at inference time: this uses the output distribution to draw a sample of a given size, as a way to encode the predicted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_scaling(context, min_scale=1e-5):\n",
    "    return context.abs().mean(1).clamp(min_scale, None).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_scaling(context):\n",
    "    return torch.ones(context.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingFeedForwardNetwork(nn.Module):\n",
    "    distr_type = torch.distributions.StudentT\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        prediction_length: int,\n",
    "        context_length: int,\n",
    "        hidden_dimensions: List[int],\n",
    "        batch_norm: bool=False,\n",
    "        scaling: Callable=mean_abs_scaling,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        assert prediction_length > 0\n",
    "        assert context_length > 0\n",
    "        assert len(hidden_dimensions) > 0\n",
    "        \n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length\n",
    "        self.hidden_dimensions = hidden_dimensions\n",
    "        self.batch_norm = batch_norm\n",
    "        self.scaling = scaling\n",
    "        \n",
    "        dimensions = [context_length] + hidden_dimensions[:-1]\n",
    "\n",
    "        modules = []\n",
    "        for in_size, out_size in zip(dimensions[:-1], dimensions[1:]):\n",
    "            modules += [self._linear_layer(in_size, out_size), nn.ReLU()]\n",
    "            if batch_norm:\n",
    "                modules.append(nn.BatchNorm1d(units))\n",
    "        modules.append(self._linear_layer(dimensions[-1], prediction_length * hidden_dimensions[-1]))\n",
    "        self.nn = nn.Sequential(*modules)\n",
    "        \n",
    "        self.df_proj = nn.Sequential(self._linear_layer(hidden_dimensions[-1], 1), nn.Softplus())\n",
    "        self.loc_proj = self._linear_layer(hidden_dimensions[-1], 1)\n",
    "        self.scale_proj = nn.Sequential(self._linear_layer(hidden_dimensions[-1], 1), nn.Softplus())\n",
    "    \n",
    "    @staticmethod\n",
    "    def _linear_layer(dim_in, dim_out):\n",
    "        lin = nn.Linear(dim_in, dim_out)\n",
    "        torch.nn.init.uniform_(lin.weight, -0.07, 0.07)\n",
    "        torch.nn.init.zeros_(lin.bias)\n",
    "        return lin\n",
    "    \n",
    "    def distr_and_scale(self, context):\n",
    "        scale = self.scaling(context)\n",
    "        scaled_context = context / scale\n",
    "        nn_out = self.nn(scaled_context)\n",
    "        nn_out_reshaped = nn_out.reshape(-1, self.prediction_length, self.hidden_dimensions[-1])\n",
    "        \n",
    "        distr_args = (\n",
    "            2.0 + self.df_proj(nn_out_reshaped).squeeze(dim=-1),\n",
    "            self.loc_proj(nn_out_reshaped).squeeze(dim=-1),\n",
    "            self.scale_proj(nn_out_reshaped).squeeze(dim=-1),\n",
    "        )\n",
    "        distr = net.distr_type(*distr_args)\n",
    "        \n",
    "        return distr, scale\n",
    "    \n",
    "    def forward(self, context, target):\n",
    "        assert context.shape[-1] == self.context_length\n",
    "        assert target.shape[-1] == self.prediction_length\n",
    "        \n",
    "        distr, scale = self.distr_and_scale(context)\n",
    "        loss = (-distr.log_prob(target / scale) + torch.log(scale)).mean(dim=1)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the inputs:\n",
    "* `context` has size `batch_size x context_length`\n",
    "* `target` has size `batch_size x prediction_length`\n",
    "\n",
    "Shape of the output:\n",
    "* `loss` has size `batch_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingFeedForwardNetwork(TrainingFeedForwardNetwork):\n",
    "    def __init__(self, *args, num_samples: int = 1000, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "    def forward(self, context):\n",
    "        assert context.shape[-1] == self.context_length\n",
    "        \n",
    "        distr, scale = self.distr_and_scale(context)\n",
    "        sample_perm = distr.sample((self.num_samples, )) * scale\n",
    "        sample = sample_perm.permute(1, 0, 2)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the inputs:\n",
    "* `context` has size `batch_size x context_length`\n",
    "\n",
    "Shape of the output:\n",
    "* `sample` has size `batch_size x num_samples x prediction_length`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate the training network, and explore its set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 2 * 7 * 24\n",
    "prediction_length = dataset.metadata.prediction_length\n",
    "hidden_dimensions = [96, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TrainingFeedForwardNetwork(\n",
    "    prediction_length=prediction_length,\n",
    "    context_length=context_length,\n",
    "    hidden_dimensions=hidden_dimensions,\n",
    "    batch_norm=False,\n",
    "    scaling=mean_abs_scaling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.prod(p.shape) for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in net.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the data loader which will yield batches of data to train on. Starting from the original dataset, the data loader is configured to apply the following transformation, which does essentially two things:\n",
    "* Replaces `nan`s in the target field with a dummy value (zero), and adds a field indicating which values were actually observed vs imputed this way.\n",
    "* Slices out training instances of a fixed length randomly from the given dataset; these will be stacked into batches by the data loader itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.loader import TrainDataLoader\n",
    "from gluonts.torch.batchify import batchify\n",
    "from gluonts.torch.support.util import copy_parameters\n",
    "from gluonts.torch.model.predictor import PyTorchPredictor\n",
    "from gluonts.transform import Chain, AddObservedValuesIndicator, InstanceSplitter, ExpectedNumInstanceSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = Chain([\n",
    "    AddObservedValuesIndicator(\n",
    "        target_field=FieldName.TARGET,\n",
    "        output_field=FieldName.OBSERVED_VALUES,\n",
    "    ),\n",
    "    InstanceSplitter(\n",
    "        target_field=FieldName.TARGET,\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        train_sampler=ExpectedNumInstanceSampler(num_instances=1),\n",
    "        past_length=context_length,\n",
    "        future_length=prediction_length,\n",
    "        time_series_fields=[FieldName.OBSERVED_VALUES],\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/training_data_iter.png\" style=\"width: 800px; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_batches_per_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = TrainDataLoader(\n",
    "    dataset.train,\n",
    "    transform=transformation,\n",
    "    batch_size=batch_size,\n",
    "    stack_fn=batchify,\n",
    "    num_batches_per_epoch=num_batches_per_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"past_target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"past_target\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"past_observed_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"future_target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"future_target\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model using any of the available optimizers from PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "for epoch_no in range(20):\n",
    "    sum_epoch_loss = 0.0\n",
    "    for batch_no, batch in enumerate(data_loader, start=1):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_vec = net(context=batch[\"past_target\"], target=batch[\"future_target\"])\n",
    "        loss = loss_vec.mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        sum_epoch_loss += loss.detach().numpy().item()\n",
    "        \n",
    "    print(f\"{epoch_no}: {sum_epoch_loss / num_batches_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictor out of the trained model, and test it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a trained model, whose parameters can be copied over to a `SamplingFeedForwardNetwork` object: we will wrap this into a `PyTorchPredictor` that can be used for inference tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_net = SamplingFeedForwardNetwork(\n",
    "    prediction_length=net.prediction_length,\n",
    "    context_length=net.context_length,\n",
    "    hidden_dimensions=net.hidden_dimensions,\n",
    "    batch_norm=net.batch_norm,\n",
    ")\n",
    "copy_parameters(net, pred_net)\n",
    "\n",
    "feedforward = PyTorchPredictor(\n",
    "    prediction_length=prediction_length, freq = dataset.metadata.freq, \n",
    "    input_names = [\"past_target\"], prediction_net=pred_net, batch_size=32, input_transform=transformation,\n",
    "    device=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can do backtesting on the test dataset: in what follows, `make_evaluation_predictions` will slice out the trailing `prediction_length` observations from the test time series, and use the given predictor to obtain forecasts for the same time range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/prediction_data_iter.png\" style=\"width: 800px; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset.test,\n",
    "    predictor=feedforward,\n",
    "    num_samples=1000,\n",
    ")\n",
    "\n",
    "forecasts_feedforward = list(forecast_it)\n",
    "tss_feedforward = list(ts_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the forecasts, we can plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formatter = matplotlib.dates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts_feedforward, tss_feedforward)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "    plt.plot(ts[-5 * prediction_length:], label=\"target\")\n",
    "    forecast.plot(color=\"g\")\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formatter)\n",
    "    \n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compute evaluation metrics, that summarize the performance of the model on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_metrics_feedforward, entrywise_metrics_feedforward = evaluator(\n",
    "    iter(tss_feedforward), iter(forecasts_feedforward), num_series=len(dataset.test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(\n",
    "    [aggregate_metrics_naive, aggregate_metrics_feedforward],\n",
    "    index=[\"Naive seasonal\", \"Feed-forward\"]\n",
    ").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using pre-implemented models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model.seq2seq import MQCNNEstimator\n",
    "from gluonts.mx.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MQ-CNN (Wen et al., 2017) is a *sequence-to-sequence* model, using\n",
    "* a convolutional network as *encoder*\n",
    "* a feed-forward network as *decoder*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/mq_rnn.png\" style=\"width: 600px; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the decoder consists of pre-defined *quantiles* of the predicted distribution, optimized using the *quantile loss* (also known as *pinball loss*):\n",
    "\n",
    "$$ L_\\alpha(\\hat z, z) = \\begin{cases} \\alpha (\\hat z - z) & \\mbox{if}\\ \\hat z \\geq z \\\\ (\\alpha - 1) (\\hat z - z) & \\mbox{otherwise} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mqcnn_estimator = MQCNNEstimator(\n",
    "    freq=dataset.metadata.freq,\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    quantiles=[0.1, 0.5, 0.9],\n",
    "    trainer=Trainer(epochs=20),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mqcnn = mqcnn_estimator.train(dataset.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mqcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset.test,\n",
    "    predictor=mqcnn,\n",
    "    num_samples=100,\n",
    ")\n",
    "\n",
    "forecasts_mqcnn = list(forecast_it)\n",
    "tss_mqcnn = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "date_formatter = matplotlib.dates.DateFormatter('%b, %d')\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for idx, (forecast, ts) in islice(enumerate(zip(forecasts_mqcnn, tss_mqcnn)), 9):\n",
    "    ax = plt.subplot(3, 3, idx+1)\n",
    "    \n",
    "    plt.plot(ts[-5 * prediction_length:], label=\"target\")\n",
    "    forecast.plot(color=\"g\")\n",
    "    plt.xticks(rotation=60)\n",
    "    ax.xaxis.set_major_formatter(date_formatter)\n",
    "    \n",
    "plt.gcf().tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_metrics_mqcnn, entrywise_metrics_mqcnn = evaluator(\n",
    "    iter(tss_mqcnn), iter(forecasts_mqcnn), num_series=len(dataset.test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(\n",
    "    [aggregate_metrics_naive, aggregate_metrics_feedforward, aggregate_metrics_mqcnn],\n",
    "    index=[\"Naive seasonal\", \"Feed-forward\", \"MQ-CNN\"]\n",
    ").transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several additional features in GluonTS we did not cover here:\n",
    "\n",
    "* Models:\n",
    "    * Wavenet [van den Oord et al., 2016]\n",
    "    * Deep State Space Models [Rangapuram et al., 2018]\n",
    "    * Deep Factors [Wang et al. 2019]\n",
    "    * DeepAR [Flunkert et al., 2020]\n",
    "    * Attention-based models (transformers)\n",
    "    * Gaussian processes, Temporal point processes, ...\n",
    "* Training options:\n",
    "    * Monitoring validation loss\n",
    "    * Early stopping\n",
    "    * Model averaging\n",
    "* Serialization/deserialization of models\n",
    "* Helpers to train & deploy models in the cloud, using Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isf2020",
   "language": "python",
   "name": "isf2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
